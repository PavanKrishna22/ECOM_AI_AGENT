Here is a comprehensive guide to setting up and running this project

### **Part 1: Prerequisites and Downloads**

Before you run the code, you need to install Python and Ollama, and download the required AI model.

#### **1. Install Python**
If you don't have Python installed, download it from the official website.

*   **Download Link:** [python.org](https://www.python.org/downloads/)

#### **2. Install and Set Up Ollama**
Ollama is the engine that will run the local AI model.

*   **Download Link:** [ollama.ai](https://ollama.ai/)
*   **Action:** Download and install the Ollama application for your operating system (macOS, Windows, or Linux).

#### **3. Download the AI Model**
The code is configured to use a custom model named `sqlcoder-custom`. The line `FROM ./sqlcoder-7b-q5_k_m.gguf` in your notes indicates that this custom model is created from a specific model file.

*   **Action Steps:**
    1.  First, download the base model file: `sqlcoder-7b-q5_k_m.gguf` from : .
    2.  Create a file named `Modelfile` (with no extension) in your project directory.
    3.  Put the following line in the `Modelfile`:
        ```
        FROM ./sqlcoder-7b-q5_k_m.gguf
        ```
    4.  Open your terminal or command prompt and run the following command to create the custom model in Ollama:
        ```bash
        ollama create sqlcoder-custom -f Modelfile
        ```    5.  Finally, run the model once to make sure it's loaded and ready:
        ```bash
        ollama run sqlcoder-custom
        ```

---

### **Part 2: Project Setup**

Now, let's set up your project directory and install the Python libraries.

1.  **Create a Project Folder:** Create a new folder for your project and place the following files inside it:
    *   `api.py` (your FastAPI code)
    *   `app.py` (your Streamlit code)
    *   `create_db.py` (your database creation script)
    *   `requirements.txt` (created above)
    *   `Modelfile` (created above)
    *   `sqlcoder-7b-q5_k_m.gguf` (the model file you downloaded)
    *   `Product-Level Ad Sales and Metrics.csv`
    *   `Product-Level Eligibility Table.csv`
    *   `Product-Level Total Sales and Metrics.csv`

2.  **Create a Virtual Environment (Recommended):** Open your terminal, navigate to your project folder, and run the following commands:
    ```bash
    # Create a virtual environment
    python -m venv venv

    # Activate it
    # On Windows:
    venv\Scripts\activate
    # On macOS/Linux:
    source venv/bin/activate
    ```

3.  **Install Required Libraries:** With your virtual environment active, install all the libraries from your `requirements.txt` file using pip:
    ```bash
    pip install -r requirements.txt
    ```

---

### **Part 3: Execution Steps**

Follow these steps in order to run your application.

#### **Step 1: Create the Database**

First, run the `create_db.py` script to generate the `ecommerce.db` database file from your CSVs.

*   **Command:**
    ```bash
    python create_db.py
    ```
*   **Expected Output:** You will see messages indicating that the tables have been created, and a new file named `ecommerce.db` will appear in your project folder.

#### **Step 2: Run the FastAPI API**

This API will handle the backend logic and communication with the AI model.

*   **Command:**
    ```bash
    uvicorn api:app --host 0.0.0.0 --port 8000
    ```
*   **Expected Output:** The terminal will show that the application has started. You can access the API documentation at `http://127.0.0.1:8000/docs` in your browser. Keep this terminal window open.
    then use this cmd to use the API end point:
    "Invoke-RestMethod -Uri "http://127.0.0.1:8000/query" -Method Post -ContentType "application/json" -Body '{"question": "Which product had the highest CPC?"}'"

#### **Step 3: Run the Streamlit Web Interface**

This is the user interface where you will interact with the data agent.

*   **Action:** Open a **new** terminal window and navigate to your project folder (and activate the virtual environment again if needed).
*   **Command:**
    ```bash
    streamlit run app.py
    ```
*   **Expected Output:** A new tab will open in your web browser with the AI Data Analyst interface, ready for you to ask questions.












python check_schema.py  # gives you the DB info

--- Schema for database: ecommerce.db ---

Table: 'ad_sales'
-----------------
Column Name               | Data Type
----------------------------------------
date                      | TEXT
item_id                   | INTEGER
ad_sales                  | REAL
impressions               | INTEGER
ad_spend                  | REAL
clicks                    | INTEGER
units_sold                | INTEGER

Table: 'eligibility'
--------------------
Column Name               | Data Type
----------------------------------------
eligibility_datetime_utc  | TEXT
item_id                   | INTEGER
eligibility               | INTEGER
message                   | TEXT

Table: 'total_sales'
--------------------
Column Name               | Data Type
----------------------------------------
date                      | TEXT
item_id                   | INTEGER
total_sales               | REAL
total_units_ordered       | INTEGER
